{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a80a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up dependencies...\n",
      "‚úì All modules loaded successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Setting up dependencies...\")\n",
    "import subprocess\n",
    "import sys\n",
    "def install_packages():\n",
    "   packages = ['sentence-transformers', 'transformers', 'torch', 'faiss-cpu', 'numpy', 'accelerate']\n",
    "   for package in packages:\n",
    "       print(f\"Installing {package}...\")\n",
    "       subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "try:\n",
    "   import faiss\n",
    "except ImportError:\n",
    "   install_packages()\n",
    "   print(\"‚úì All dependencies installed! Importing modules...\\n\")\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import faiss\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úì All modules loaded successfully!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84956b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "   def __init__(self, embedding_model='all-MiniLM-L6-v2'):\n",
    "       print(f\"Loading embedding model: {embedding_model}...\")\n",
    "       self.embedder = SentenceTransformer(embedding_model)\n",
    "       self.documents = []\n",
    "       self.index = None\n",
    "   def add_documents(self, docs: List[str], sources: List[str]):\n",
    "       self.documents = [{\"text\": doc, \"source\": src} for doc, src in zip(docs, sources)]\n",
    "       embeddings = self.embedder.encode(docs, show_progress_bar=False)\n",
    "       dimension = embeddings.shape[1]\n",
    "       self.index = faiss.IndexFlatL2(dimension)\n",
    "       self.index.add(embeddings.astype('float32'))\n",
    "       print(f\"‚úì Indexed {len(docs)} documents\\n\")\n",
    "   def search(self, query: str, k: int = 3) -> List[Dict]:\n",
    "       query_vec = self.embedder.encode([query]).astype('float32')\n",
    "       distances, indices = self.index.search(query_vec, k)\n",
    "       return [self.documents[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a07955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryRouter:\n",
    "   def __init__(self):\n",
    "       self.categories = {\n",
    "           'technical': ['how', 'implement', 'code', 'function', 'algorithm', 'debug'],\n",
    "           'factual': ['what', 'who', 'when', 'where', 'define', 'explain'],\n",
    "           'comparative': ['compare', 'difference', 'versus', 'vs', 'better', 'which'],\n",
    "           'procedural': ['steps', 'process', 'guide', 'tutorial', 'how to']\n",
    "       }\n",
    "   def route(self, query: str) -> str:\n",
    "       query_lower = query.lower()\n",
    "       scores = {}\n",
    "       for category, keywords in self.categories.items():\n",
    "           score = sum(1 for kw in keywords if kw in query_lower)\n",
    "           scores[category] = score\n",
    "       best_category = max(scores, key=scores.get)\n",
    "       return best_category if scores[best_category] > 0 else 'factual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf521e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(self, model_name='google/flan-t5-base'):\n",
    "        print(f\"Loading generation model: {model_name}...\")\n",
    "        self.generator = pipeline(\n",
    "            'text2text-generation',\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            max_length=256\n",
    "        )\n",
    "        device_type = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "        print(f\"‚úì Generator ready (using {device_type})\\n\")\n",
    "\n",
    "    def generate(self, query: str, context: List[Dict], query_type: str) -> str:\n",
    "        context_text = \"\\n\\n\".join([f\"[{doc['source']}]: {doc['text']}\" for doc in context])\n",
    "\n",
    "        # ‚úÖ Properly close and assign the triple-quoted prompt string\n",
    "        prompt = f\"\"\"Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        answer = self.generator(prompt, max_length=200, do_sample=False)[0]['generated_text']\n",
    "        return answer.strip()\n",
    "\n",
    "    def self_check(self, query: str, answer: str, context: List[Dict]) -> Tuple[bool, str]:\n",
    "        if len(answer) < 10:\n",
    "            return False, \"Answer too short - needs more detail\"\n",
    "\n",
    "        context_keywords = set()\n",
    "        for doc in context:\n",
    "            context_keywords.update(doc['text'].lower().split()[:20])\n",
    "\n",
    "        answer_words = set(answer.lower().split())\n",
    "        overlap = len(context_keywords.intersection(answer_words))\n",
    "\n",
    "        if overlap < 2:\n",
    "            return False, \"Answer not grounded in context - needs more evidence\"\n",
    "\n",
    "        query_keywords = set(query.lower().split())\n",
    "        if len(query_keywords.intersection(answer_words)) < 1:\n",
    "            return False, \"Answer doesn't address the query - rephrase needed\"\n",
    "\n",
    "        return True, \"Answer quality acceptable\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1ba08fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgenticRAG:\n",
    "   def __init__(self):\n",
    "       self.vector_store = VectorStore()\n",
    "       self.router = QueryRouter()\n",
    "       self.generator = AnswerGenerator()\n",
    "       self.max_iterations = 2\n",
    "   def add_knowledge(self, documents: List[str], sources: List[str]):\n",
    "       self.vector_store.add_documents(documents, sources)\n",
    "   def query(self, question: str, verbose: bool = True) -> Dict:\n",
    "       if verbose:\n",
    "           print(f\"\\n{'='*60}\")\n",
    "           print(f\"ü§î Query: {question}\")\n",
    "           print(f\"{'='*60}\")\n",
    "       query_type = self.router.route(question)\n",
    "       if verbose:\n",
    "           print(f\"üìç Route: {query_type.upper()} query detected\")\n",
    "       k_docs = {'technical': 2, 'comparative': 4, 'procedural': 3}.get(query_type, 3)\n",
    "       iteration = 0\n",
    "       answer_accepted = False\n",
    "       while iteration < self.max_iterations and not answer_accepted:\n",
    "           iteration += 1\n",
    "           if verbose:\n",
    "               print(f\"\\nüîÑ Iteration {iteration}\")\n",
    "           context = self.vector_store.search(question, k=k_docs)\n",
    "           if verbose:\n",
    "               print(f\"üìö Retrieved {len(context)} documents from sources:\")\n",
    "               for doc in context:\n",
    "                   print(f\"   - {doc['source']}\")\n",
    "           answer = self.generator.generate(question, context, query_type)\n",
    "           if verbose:\n",
    "               print(f\"üí° Generated answer: {answer[:100]}...\")\n",
    "           answer_accepted, feedback = self.generator.self_check(question, answer, context)\n",
    "           if verbose:\n",
    "               status = \"‚úì ACCEPTED\" if answer_accepted else \"‚úó REJECTED\"\n",
    "               print(f\"üîç Self-check: {status}\")\n",
    "               print(f\"   Feedback: {feedback}\")\n",
    "           if not answer_accepted and iteration < self.max_iterations:\n",
    "               question = f\"{question} (provide more specific details)\"\n",
    "               k_docs += 1\n",
    "       return {'answer': answer, 'query_type': query_type, 'iterations': iteration, 'accepted': answer_accepted, 'sources': [doc['source'] for doc in context]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0be0c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ AGENTIC RAG WITH ROUTING & SELF-CHECK\n",
      "============================================================\n",
      "\n",
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Loading generation model: google/flan-t5-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Generator ready (using CPU)\n",
      "\n",
      "‚úì Indexed 1 documents\n",
      "\n",
      "\n",
      "============================================================\n",
      "ü§î Query: What is Python?\n",
      "============================================================\n",
      "üìç Route: FACTUAL query detected\n",
      "\n",
      "üîÑ Iteration 1\n",
      "üìö Retrieved 3 documents from sources:\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "üí° Generated answer: Python is a programming language....\n",
      "üîç Self-check: ‚úó REJECTED\n",
      "   Feedback: Answer not grounded in context - needs more evidence\n",
      "\n",
      "üîÑ Iteration 2\n",
      "üìö Retrieved 4 documents from sources:\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "üí° Generated answer: Python is a programming language....\n",
      "üîç Self-check: ‚úó REJECTED\n",
      "   Feedback: Answer not grounded in context - needs more evidence\n",
      "\n",
      "============================================================\n",
      "üìä FINAL RESULT:\n",
      "   Answer: Python is a programming language.\n",
      "   Query Type: factual\n",
      "   Iterations: 2\n",
      "   Accepted: False\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ü§î Query: How does machine learning work?\n",
      "============================================================\n",
      "üìç Route: TECHNICAL query detected\n",
      "\n",
      "üîÑ Iteration 1\n",
      "üìö Retrieved 2 documents from sources:\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "üí° Generated answer: RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. It retriev...\n",
      "üîç Self-check: ‚úó REJECTED\n",
      "   Feedback: Answer doesn't address the query - rephrase needed\n",
      "\n",
      "üîÑ Iteration 2\n",
      "üìö Retrieved 3 documents from sources:\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "üí° Generated answer: RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. It retriev...\n",
      "üîç Self-check: ‚úó REJECTED\n",
      "   Feedback: Answer doesn't address the query - rephrase needed\n",
      "\n",
      "============================================================\n",
      "üìä FINAL RESULT:\n",
      "   Answer: RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. It retrieves relevant documents and uses them as context for generating accurate answers.\n",
      "   Query Type: technical\n",
      "   Iterations: 2\n",
      "   Accepted: False\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ü§î Query: Compare neural networks and deep learning\n",
      "============================================================\n",
      "üìç Route: COMPARATIVE query detected\n",
      "\n",
      "üîÑ Iteration 1\n",
      "üìö Retrieved 4 documents from sources:\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "üí° Generated answer: Neural Networks vs. Deep Learning...\n",
      "üîç Self-check: ‚úó REJECTED\n",
      "   Feedback: Answer not grounded in context - needs more evidence\n",
      "\n",
      "üîÑ Iteration 2\n",
      "üìö Retrieved 5 documents from sources:\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "   - Python Documentation\n",
      "üí° Generated answer: Neural networks and deep learning (provide more specific details)...\n",
      "üîç Self-check: ‚úó REJECTED\n",
      "   Feedback: Answer not grounded in context - needs more evidence\n",
      "\n",
      "============================================================\n",
      "üìä FINAL RESULT:\n",
      "   Answer: Neural networks and deep learning (provide more specific details)\n",
      "   Query Type: comparative\n",
      "   Iterations: 2\n",
      "   Accepted: False\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "   print(\"\\n\" + \"=\"*60)\n",
    "   print(\"üöÄ AGENTIC RAG WITH ROUTING & SELF-CHECK\")\n",
    "   print(\"=\"*60 + \"\\n\")\n",
    "   documents = [\n",
    "       \"RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. It retrieves relevant documents and uses them as context for generating accurate answers.\"\n",
    "   ]\n",
    "   sources = [\"Python Documentation\", \"ML Textbook\", \"Neural Networks Guide\", \"Deep Learning Paper\", \"Transformer Architecture\", \"RAG Research Paper\"]\n",
    "   rag = AgenticRAG()\n",
    "   rag.add_knowledge(documents, sources)\n",
    "   test_queries = [\"What is Python?\", \"How does machine learning work?\", \"Compare neural networks and deep learning\"]\n",
    "   for query in test_queries:\n",
    "       result = rag.query(query, verbose=True)\n",
    "       print(f\"\\n{'='*60}\")\n",
    "       print(f\"üìä FINAL RESULT:\")\n",
    "       print(f\"   Answer: {result['answer']}\")\n",
    "       print(f\"   Query Type: {result['query_type']}\")\n",
    "       print(f\"   Iterations: {result['iterations']}\")\n",
    "       print(f\"   Accepted: {result['accepted']}\")\n",
    "       print(f\"{'='*60}\\n\")\n",
    "if __name__ == \"__main__\":\n",
    "   main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
